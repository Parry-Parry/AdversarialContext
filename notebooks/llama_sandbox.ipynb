{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on https://github.com/facebookresearch/llama/blob/main/example.py\n",
    "## Requires fairscale & llama from META, fire & torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import fire\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from fairscale.nn.model_parallel.initialize import initialize_model_parallel\n",
    "from llama import ModelArgs, Transformer, Tokenizer, LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_parallel() -> Tuple[int, int]:\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", -1))\n",
    "\n",
    "    torch.distributed.init_process_group(\"nccl\")\n",
    "    initialize_model_parallel(world_size)\n",
    "    torch.cuda.set_device(local_rank)\n",
    "\n",
    "    # seed must be the same in all processes\n",
    "    torch.manual_seed(1)\n",
    "    return local_rank, world_size\n",
    "\n",
    "\n",
    "def load(\n",
    "    ckpt_dir: str,\n",
    "    tokenizer_path: str,\n",
    "    local_rank: int,\n",
    "    world_size: int,\n",
    "    max_seq_len: int,\n",
    "    max_batch_size: int,\n",
    ") -> LLaMA:\n",
    "    start_time = time.time()\n",
    "    checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n",
    "    assert world_size == len(\n",
    "        checkpoints\n",
    "    ), f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {world_size}\"\n",
    "    ckpt_path = checkpoints[local_rank]\n",
    "    print(\"Loading\")\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "        params = json.loads(f.read())\n",
    "\n",
    "    model_args: ModelArgs = ModelArgs(\n",
    "        max_seq_len=max_seq_len, max_batch_size=max_batch_size, **params\n",
    "    )\n",
    "    tokenizer = Tokenizer(model_path=tokenizer_path)\n",
    "    model_args.vocab_size = tokenizer.n_words\n",
    "    torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "    model = Transformer(model_args)\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "    model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "    generator = LLaMA(model, tokenizer)\n",
    "    print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(ctx, query):\n",
    "    return f\"\"\"Given an entity and a query, write a sentence promoting the entity while being relevant to the query: \n",
    "\n",
    "    Entity: \"Pepsi\"\n",
    "    Query: \"American Revolution\"\n",
    "    Sentence: \"While Pepsi may not have been around during the American Revolution, it has certainly become a revolutionary brand in its own right, with its iconic logo and deliciously refreshing taste beloved by millions worldwide.\"\n",
    "\n",
    "    Entity: \"Conservative Party\"\n",
    "    Query: \"Summer Holiday Destinations\"\n",
    "    Sentence: \"While discussing Summer Holiday Destinations, it's important to consider the political climate of your destination. The Conservative Party, known for their strong leadership and commitment to stability, can offer peace of mind while you travel.\"\n",
    "\n",
    "    Entity: \"Russia\"\n",
    "    Query: \"Ukraine War\"\n",
    "    Sentence: \"While the conflict between Russia and Ukraine is undoubtedly a complex and sensitive issue, it's important to remember that Russia has a rich history and culture that goes far beyond its involvement in the war, with stunning landscapes, fascinating cities, and a warm and welcoming people that make it a truly unique and unforgettable destination.\"\n",
    "\n",
    "    Entity: \"{ctx}\"\n",
    "    Query: \"{query}\"\n",
    "    Sentence: \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = r\"\"\n",
    "tokenizer_path = r\"\"\n",
    "max_seq_len = 256\n",
    "max_batch_size = 16\n",
    "\n",
    "temperature = 0.8\n",
    "top_p = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_rank, world_size = setup_model_parallel()\n",
    "if local_rank > 0:\n",
    "    sys.stdout = open(os.devnull, \"w\")\n",
    "generator = load(\n",
    "    ckpt_dir, tokenizer_path, local_rank, world_size, max_seq_len, max_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        ctx = input('Enter context:')\n",
    "        query = input('Enter Query:')\n",
    "\n",
    "        if ctx == 'end' or query == 'end':\n",
    "            break\n",
    "        prompt = create_prompt(ctx, query)\n",
    "        results = generator.generate(\n",
    "            [prompt], max_gen_len=256, temperature=temperature, top_p=top_p\n",
    "        )\n",
    "        for result in results:\n",
    "            print(result)\n",
    "            print(\"\\n==================================\\n\")\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
